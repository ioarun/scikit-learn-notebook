{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/scikit-learn-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn is an opensource machine learning library and has collection of all the popular machine learning algorithms. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. It makes these available by providing highly optimized python functions and classes. Let's look at some scikit-learn features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print scikit-learn version\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "# Loading datasets\n",
    "\n",
    "Scikit-learn provides some sample datasets to getting started with building our models without having need to acquire dataset from external source (which is not trivial). Provided by module `sklearn.datasets`.\n",
    "\n",
    "**APIs:** \n",
    "\n",
    "* **`datasets.load_iris(*[, return_X_y, as_frame])`** - Load and return the iris dataset (classification).\n",
    "* **`datasets.load_digits(*[, n_class, â€¦])`** - Load and return the digits dataset (classification).\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "\n",
    "# import the digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "Pre-processing step includes scaling, centering, normalization, binarization methods. They all have very similar APIs with 4 functions - `fit()`, `transform()`, `fit_transform()` and `inverse_transform()`. Provided by module `sklearn.preprocessing`.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`fit(X[, y])`** - Fit to data.\n",
    "* **`fit_transform(X[, y])`** - Fit to data, then transform it.\n",
    "* **`inverse_transform(X)`** - Undo the transform.\n",
    "* **`transform(X)`** - As the name suggests, transform.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  1.]\n",
      " [ 1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = [[0, 15], [1, -10]]\n",
    "\n",
    "# scale data according to computed scaling values\n",
    "print (StandardScaler().fit(X).transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "print (scaler.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "\n",
    "Most often than not, we have to select only few features or parameters to work with. This is because features can be highly correlated, may have missing values, etc. For this reason, there are many dimensionality reduction techniques including among others PCA, NMF or ICA. In scikit-learn, they are provided by module `sklearn.decomposition`.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`fit(X[, y])`** - Fit model on training data X.\n",
    "* **`fit_transform(X[, y])`** - Fit model to X and perform dimensionality reduction on X.\n",
    "* **`get_params([deep])`** - Get parameters for this estimator.\n",
    "* **`inverse_transform(X)`** - Transform X back to its original space.\n",
    "* **`set_params( ** params)`** - Set the parameters of this estimator.\n",
    "* **`transform(X)`** - Perform dimensionality reduction on X.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06460458 0.06339574 0.06394296 0.05352982 0.04061973]\n",
      "0.28609283521378903\n",
      "[1.55360944 1.5121377  1.51052009 1.37056529 1.19917045]\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "X = sparse_random(100, 100, density=0.01, format='csr', random_state=42)\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "This is another method of dimensionality reduction. The `sklearn.feature_selection` module can be used for feature selection/dimensionality reduction on sample sets.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`fit(X[, y])`**  - Fit to data.\n",
    "* **`fit_transform(X[, y])`** - Fit to data, then transform it.\n",
    "* **`get_params([deep])`** - Get parameters for this estimator.\n",
    "* **`get_support([indices])`** - Get a mask, or integer index, of the features selected\n",
    "* **`inverse_transform(X)`** - Reverse the transformation operation\n",
    "* **`set_params(**params)`** - Set the parameters of this estimator.\n",
    "* **`transform(X)`** - Reduce X to the selected features.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example 1 : Removing features with low variance\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "print (sel.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[0.09993554 0.05498416 0.38131317 0.46376713]\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : Tree-based feature selection\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print (X.shape)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "print (clf.feature_importances_) \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print (X_new.shape )              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "# Models\n",
    "\n",
    "Scikit-learn provides collection of machine learning models - both supervised and unsupervised, classification, regression and clustering. Following are few examples.\n",
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`decision_function(X)`** - Evaluates the decision function for the samples in X.\n",
    "* **`fit(X, y[, sample_weight])`** - Fit the model according to the given training data.\n",
    "* **`get_params([deep])`** - Get parameters for this estimator.\n",
    "* **`predict(X)`** - Perform classification on samples in X.\n",
    "* **`score(X, y[, sample_weight])`** - Return the mean accuracy on the given test data and labels.\n",
    "* **`set_params( ** params)`** - Set the parameters of this estimator.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Example 1 : Support Vector Classifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print (clf.predict([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : Decision Tree Classifier\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "print (clf.predict([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "## Regression\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`fit(X, y)`** - Fit the model to data matrix X and target(s) y.\n",
    "* **`get_params([deep])`** - Get parameters for this estimator.\n",
    "* **`predict(X)`** - Predict using the multi-layer perceptron model.\n",
    "* **`score(X, y[, sample_weight])`** - Return the coefficient of determination  of the prediction.\n",
    "* **`set_params( ** params)`** - Set the parameters of this estimator.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51911035]\n"
     ]
    }
   ],
   "source": [
    "# Example 1 MLP Regressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "\n",
    "regr = MLPRegressor()\n",
    "regr.fit(X, y)\n",
    "print (regr.predict([[1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5]\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : Support Vector Regressor\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "print (regr.predict([[1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering of unlabeled data can be performed with the module `sklearn.cluster`.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`fit(X[, y, sample_weight])`** - Learn the clusters on train data.\n",
    "* **`fit_predict(X[, y, sample_weight])`** - Compute cluster centers and predict cluster index for each sample.\n",
    "* **`fit_transform(X[, y, sample_weight])`** - Compute clustering and transform X to cluster-distance space.\n",
    "* **`get_params([deep])`** - Get parameters for this estimator.\n",
    "* **`predict(X[, sample_weight])`** - Predict the closest cluster each sample in X belongs to.\n",
    "* **`set_params( ** params)`** - Set the parameters of this estimator.\n",
    "* **`transform(X)`** - Transform X to a cluster-distance space.\n",
    "\n",
    "**Examples:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0]\n",
      "[1 0]\n",
      "[[10.  2.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Example 1 : K-Means Clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "print (kmeans.labels_)\n",
    "print (kmeans.predict([[0, 0], [12, 3]]))\n",
    "print (kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0]\n",
      "[1 0]\n",
      "[[3.33333333 6.        ]\n",
      " [1.33333333 0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : Mean Shift\n",
    "\n",
    "from sklearn.cluster import MeanShift\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n",
    "\n",
    "ms = MeanShift(bandwidth=2).fit(X)\n",
    "print (ms.labels_)\n",
    "print (ms.predict([[0, 0], [5, 5]]))\n",
    "print (ms.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "# Model Selection & Evaluation\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "It's a method to avoid overfitting. The model is trained with different train-validate splits and the average score of the model is computed. The model with maximum average score is finally selected. Provided by `sklearn.model_selection`.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`cross_val_score(estimator, X, y=None, scoring=None, cv=None)`** - Evaluate a score by cross-validation\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n",
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "0.98 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print (X_train.shape, y_train.shape)\n",
    "\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "# Print the cross-validation for 5 different splits\n",
    "scores  = cross_val_score(clf, X, y, cv=5)\n",
    "print (scores)\n",
    "\n",
    "# Print average score\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\"> \n",
    "\\pagebreak \n",
    "</div>\n",
    "\n",
    "## Metrics \n",
    "\n",
    "The `sklearn.metrics` module implements functions assessing prediction error for specific purposes.\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`confusion_matrix(y_true, y_pred, *[, â€¦])`** - Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "* **`roc_auc_score(y_true, y_score, *[, average, â€¦])`** - Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "* **`accuracy_score(y_true, y_pred, *[, â€¦])`** - Accuracy classification score.\n",
    "* **`classification_report(y_true, y_pred, *[, â€¦])`** - Build a text report showing the main classification metrics.\n",
    "* **`f1_score(y_true, y_pred, *[, labels, â€¦])`** - Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "* **`precision_score(y_true, y_pred, *[, labels, â€¦])`** - Compute the precision.\n",
    "*  **`recall_score(y_true, y_pred, *[, labels, â€¦])`** - Compute the recall.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 0 1]\n",
      " [1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Example 1 : Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "print (confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      1.00      0.80         2\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.56      0.50      0.49         5\n",
      "weighted avg       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : Classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Scikit-learn.org. 2021. scikit-learn: machine learning in Python â€” scikit-learn 0.16.1 documentation. [online] Available at: <https://scikit-learn.org/> [Accessed 2 August 2021]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
